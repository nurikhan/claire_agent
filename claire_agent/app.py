# claire_agent/app.py
import streamlit as st
import datetime
from typing import List

from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import HumanMessage, AIMessage, SystemMessage, BaseMessage

# ë‚´ë¶€ ëª¨ë“ˆ import
from core.config import (
    DEFAULT_OLLAMA_MODEL_NAME, 
    CURRENT_LOCATION_STR
)
from core.llm_services import (
    get_chat_llm_instance, 
    get_ollama_models_available,
    get_embedding_model # MemorySystem ì´ˆê¸°í™”ì— í•„ìš”í•  ìˆ˜ ìˆìŒ (ì§ì ‘ ì‚¬ìš©ì€ X)
)
from core.db_services import (
    init_sqlite_db, 
    get_rag_vector_store, 
    get_memory_vector_store
)
from core.memory_system import MemorySystem
from prompts.system_prompts import SYSTEM_PROMPT_CONTENT_TEMPLATE

# --- 0. ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸° ì„¤ì • ---
st.set_page_config(page_title="Claire - ê°œì¸ AI ì—ì´ì „íŠ¸", layout="wide")
st.title("Claire ğŸ§  - Personal AI Agent")

# --- 1. ì„¸ì…˜ ìƒíƒœ ë° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ---
if "app_initialized" not in st.session_state:
    init_sqlite_db() # SQLite DB í…Œì´ë¸” ìƒì„±/í™•ì¸
    st.session_state.app_initialized = True

if "selected_ollama_model" not in st.session_state:
    st.session_state.selected_ollama_model = DEFAULT_OLLAMA_MODEL_NAME
if "messages" not in st.session_state: # UI í‘œì‹œìš© ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    st.session_state.messages = [{"role": "assistant", "content": f"ì•ˆë…•í•˜ì„¸ìš”! Claireì…ë‹ˆë‹¤. (ëª¨ë¸: {st.session_state.selected_ollama_model})"}]
if "lc_memory" not in st.session_state: # LangChain ëŒ€í™” ê¸°ë¡ìš© ë©”ëª¨ë¦¬
    st.session_state.lc_memory = ConversationBufferWindowMemory(
        k=7, # ìµœê·¼ 7ê°œ í„´ ê¸°ì–µ (ì¡°ì • ê°€ëŠ¥)
        return_messages=True, 
        memory_key="history", # í”„ë¡¬í”„íŠ¸ì˜ MessagesPlaceholderì™€ ì¼ì¹˜
        input_key="human_input" # í”„ë¡¬í”„íŠ¸ì˜ HumanMessage contentì™€ ì¼ì¹˜
    )
if "current_session_id" not in st.session_state:
    st.session_state.current_session_id = datetime.datetime.now().strftime("session_%Y%m%d%H%M%S_%f")
if "user_profile" not in st.session_state:
    st.session_state.user_profile = {"name": "ì£¼ì¸ë‹˜", "preferences_summary": "íŒŒì•…ëœ ì‚¬ìš©ì íŠ¹ì • ì„ í˜¸ ì •ë³´ ì—†ìŒ."}

# --- 2. í•µì‹¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë¡œë“œ (Streamlit ìºì‹± í™œìš©) ---
# LLM, ì„ë² ë”©, ë²¡í„°DBëŠ” Streamlitì˜ cache_resourceë¥¼ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬
embedding_model_global = get_embedding_model() # MemorySystem ìƒì„±ìì— í•„ìš”
rag_vector_store_global = get_rag_vector_store()
memory_vector_store_global = get_memory_vector_store()

# í˜„ì¬ ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ChatOllama ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
# ì´ ì¸ìŠ¤í„´ìŠ¤ëŠ” ì±„íŒ… ì‘ë‹µ ìƒì„± ë° ë©”ëª¨ë¦¬ ìš”ì•½ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
current_chat_llm = get_chat_llm_instance(st.session_state.selected_ollama_model)

# MemorySystem ì¸ìŠ¤í„´ìŠ¤ (LLM, ì„ë² ë”©, ë©”ëª¨ë¦¬ VDB ì „ë‹¬)
memory_system_instance = MemorySystem(
    llm_for_summarization=current_chat_llm, # ì±„íŒ… LLMì„ ìš”ì•½ì—ë„ ì‚¬ìš©
    embedding_instance=embedding_model_global,
    memory_vdb_instance=memory_vector_store_global
)

# --- 3. ì‚¬ì´ë“œë°” UI êµ¬ì„± ---
with st.sidebar:
    st.header("ğŸ¤– ëª¨ë¸ ë° ì‹œìŠ¤í…œ ì„¤ì •")
    
    available_ollama_models = get_ollama_models_available()
    # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ì´ ëª©ë¡ì— ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ëª¨ë¸ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
    try:
        current_model_index = available_ollama_models.index(st.session_state.selected_ollama_model)
    except ValueError:
        current_model_index = 0
        if available_ollama_models: # ëª©ë¡ì´ ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´
            st.session_state.selected_ollama_model = available_ollama_models[0]
        else: # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì „í˜€ ì—†ë‹¤ë©´ (Ollama ì„œë²„ ë¬¸ì œ ë“±)
            st.error("ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollama ì„œë²„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            # ì´ ê²½ìš° ì•± ì‹¤í–‰ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì ì ˆí•œ ì²˜ë¦¬ í•„ìš” (ì˜ˆ: ê¸°ë³¸ ëª¨ë¸ í•˜ë“œì½”ë”©)
            st.session_state.selected_ollama_model = "gemma3:4b" # ì„ì‹œ fallback

    newly_selected_model = st.selectbox(
        "Ollama ëª¨ë¸ ì„ íƒ:",
        options=available_ollama_models,
        index=current_model_index,
        key="model_selector_sidebar_app" # í‚¤ ë³€ê²½ìœ¼ë¡œ ì¶©ëŒ ë°©ì§€
    )

    if newly_selected_model != st.session_state.selected_ollama_model:
        st.session_state.selected_ollama_model = newly_selected_model
        # ëª¨ë¸ ë³€ê²½ ì‹œ ê´€ë ¨ ìºì‹œëœ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
        get_chat_llm_instance.clear() 
        # MemorySystemì€ ë‚´ë¶€ì ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ë¯€ë¡œ, LLMì´ ë°”ë€Œë©´ MemorySystemë„ ë‹¤ì‹œ ë§Œë“¤ì–´ì•¼ í•¨
        # í•˜ì§€ë§Œ @st.cache_resourceëŠ” ì¸ìê°€ ë°”ë€Œë©´ ìë™ìœ¼ë¡œ ì¬ìƒì„±í•˜ë¯€ë¡œ ëª…ì‹œì  clear ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” ëª…í™•ì„±ì„ ìœ„í•´ í˜¸ì¶œ (get_memory_system_instanceëŠ” ì¸ìë¡œ llmì„ ë°›ìŒ)
        st.success(f"ëª¨ë¸ì´ {newly_selected_model}ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•©ë‹ˆë‹¤.")
        st.rerun() # ë³€ê²½ì‚¬í•­ ì ìš© ë° LLM ì¸ìŠ¤í„´ìŠ¤ ì¬ìƒì„±ì„ ìœ„í•´ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨

    st.markdown(f"<sub>í˜„ì¬ LLM: **{st.session_state.selected_ollama_model}**</sub>", unsafe_allow_html=True)

    with st.expander("âš™ï¸ ì‚¬ìš©ì í”„ë¡œí•„ ë° ê¸°ì–µ ê´€ë¦¬", expanded=False):
        st.subheader("ì‚¬ìš©ì í”„ë¡œí•„")
        st.session_state.user_profile["name"] = st.text_input(
            "ì‚¬ìš©ì ì´ë¦„ (í˜¸ì¹­ìš©)", 
            value=st.session_state.user_profile.get("name", "ì£¼ì¸ë‹˜"), 
            key="user_name_input_sidebar_app"
        )
        st.session_state.user_profile["preferences_summary"] = st.text_area(
            "ì‚¬ìš©ì ì„ í˜¸ë„/ì •ë³´ ìš”ì•½", 
            value=st.session_state.user_profile.get("preferences_summary", ""), 
            height=100, 
            help="AIê°€ ì‚¬ìš©ìë¥¼ ë” ì˜ ì´í•´í•˜ê³  ê°œì¸í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.",
            key="user_prefs_sidebar_app"
        )

        st.subheader("ì¥ê¸° ê¸°ì–µ ê´€ë¦¬")
        use_llm_for_manual_summary = st.checkbox(
            "LLMìœ¼ë¡œ í˜„ì¬ ëŒ€í™” ìë™ ìš”ì•½ (ìˆ˜ë™ ì €ì¥ ì‹œ)", 
            value=True, 
            key="llm_summary_checkbox_sidebar_app"
        )
        user_provided_manual_summary = st.text_area(
            "ë˜ëŠ”, ì§ì ‘ ìš”ì•½ ì…ë ¥ (ìˆ˜ë™ ì €ì¥ìš©):", 
            height=100, 
            key="user_summary_input_sidebar_app", 
            help="AI ìš”ì•½ ëŒ€ì‹  ì§ì ‘ ìš”ì•½ ì…ë ¥ ì‹œ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )

        if st.button("í˜„ì¬ ëŒ€í™” ìš”ì•½ ì €ì¥ (ìˆ˜ë™)", key="save_summary_button_sidebar_app"):
            conversation_history_for_summary = "\n".join(
                [f"{m['role']}: {m['content']}" for m in st.session_state.messages if m['role'] != 'system']
            )
            if not conversation_history_for_summary.strip() and not user_provided_manual_summary.strip():
                st.warning("ìš”ì•½í•  ëŒ€í™” ë‚´ìš©ì´ ì—†ê±°ë‚˜ ì§ì ‘ ì…ë ¥ëœ ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                summary_input_for_consolidation = user_provided_manual_summary if user_provided_manual_summary.strip() else None
                with st.spinner("ì¥ê¸° ê¸°ì–µ ì €ì¥ ì¤‘..."):
                    stored_ltm_entry = memory_system_instance.consolidate_session_memory(
                        session_id=st.session_state.current_session_id,
                        conversation_history_str=conversation_history_for_summary,
                        user_provided_summary=summary_input_for_consolidation,
                        use_llm_summary=use_llm_for_manual_summary if not summary_input_for_consolidation else False
                    )
                if stored_ltm_entry:
                    st.success(f"ëŒ€í™” ìš”ì•½ (ID: {stored_ltm_entry.id})ì´ ì¥ê¸° ê¸°ì–µì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    st.error("ì¥ê¸° ê¸°ì–µ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # ìë™ ì¥ê¸° ê¸°ì–µ ì €ì¥ ê¸°ëŠ¥ í™œì„±í™” ì²´í¬ë°•ìŠ¤
        st.session_state.auto_save_ltm_checkbox = st.checkbox( # session_stateì— ì§ì ‘ í• ë‹¹
            "ë§¤ ì‘ë‹µ í›„ ìë™ ì¥ê¸° ê¸°ì–µ ì €ì¥ (LLM ìš”ì•½ ì‚¬ìš©)", 
            value=st.session_state.get("auto_save_ltm_checkbox", False), # ì´ì „ ê°’ ìœ ì§€
            key="auto_save_ltm_checkbox_app",
            help="LLM ì‘ë‹µ í›„ í˜„ì¬ ëŒ€í™” ë‚´ìš©ì„ ìë™ìœ¼ë¡œ ìš”ì•½í•˜ì—¬ ì¥ê¸° ê¸°ì–µì— ì €ì¥í•©ë‹ˆë‹¤. (ë§¤ë²ˆ LLM í˜¸ì¶œì´ ì¶”ê°€ë¡œ ë°œìƒ)"
        )

        st.subheader("ì¥ê¸° ê¸°ì–µ í”¼ë“œë°±")
        memory_id_for_feedback = st.number_input("í”¼ë“œë°±í•  ê¸°ì–µ ID (SQLite ID):", min_value=1, step=1, format="%d", key="fb_id_sidebar_app")
        new_importance_for_feedback = st.slider("ìƒˆë¡œìš´ ì¤‘ìš”ë„ ì ìˆ˜:", 0.0, 1.0, 0.5, 0.05, key="fb_imp_sidebar_app")
        # new_summary_for_feedback = st.text_area("ìˆ˜ì •ëœ ìš”ì•½ (ì„ íƒ):", height=80, key="fb_summary_sidebar_app") # í–¥í›„ í™•ì¥ ê°€ëŠ¥

        if st.button("ì„ íƒí•œ ê¸°ì–µì— í”¼ë“œë°± ì ìš©", key="fb_apply_sidebar_app"):
            if memory_id_for_feedback:
                feedback_success = memory_system_instance.apply_user_feedback_to_memory(
                    memory_sqlite_id=memory_id_for_feedback,
                    new_importance=new_importance_for_feedback
                    # new_summary=new_summary_for_feedback # ìš”ì•½ ìˆ˜ì • ê¸°ëŠ¥ ì¶”ê°€ ì‹œ
                )
                if feedback_success:
                    st.success(f"ê¸°ì–µ ID {memory_id_for_feedback}ì— í”¼ë“œë°±ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    st.error(f"ê¸°ì–µ ID {memory_id_for_feedback}ì— ëŒ€í•œ í”¼ë“œë°± ì ìš©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.warning("í”¼ë“œë°±í•  ê¸°ì–µ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

        if st.button("ì£¼ê¸°ì  ê¸°ì–µ ìœ ì§€ë³´ìˆ˜ ì‹¤í–‰", key="maint_button_sidebar_app"):
            with st.spinner("ê¸°ì–µ ìœ ì§€ë³´ìˆ˜ ì‘ì—… ì§„í–‰ ì¤‘..."):
                memory_system_instance.periodic_memory_maintenance()
            st.success("ê¸°ì–µ ìœ ì§€ë³´ìˆ˜ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    st.caption(f"í˜„ì¬ ì„¸ì…˜ ID: {st.session_state.current_session_id}")
    st.markdown("---")
    if st.button("í˜„ì¬ ëŒ€í™”ì°½ ë‚´ìš© ì§€ìš°ê¸° (ë‹¨ê¸° ê¸°ì–µ ì´ˆê¸°í™”)", key="clear_chat_button_sidebar_app"):
        st.session_state.messages = [{"role": "assistant", "content": f"Claireì…ë‹ˆë‹¤. (ëª¨ë¸: {st.session_state.selected_ollama_model})"}]
        st.session_state.lc_memory.clear() # LangChain ë©”ëª¨ë¦¬ë„ ì´ˆê¸°í™”
        # ìƒˆ ì„¸ì…˜ ID ë¶€ì—¬ (ì„ íƒ ì‚¬í•­)
        # st.session_state.current_session_id = datetime.datetime.now().strftime("session_%Y%m%d%H%M%S_%f")
        st.rerun()

# --- 4. ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
# ì´ì „ ëŒ€í™” ë‚´ìš© UIì— í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_query := st.chat_input("Claireì—ê²Œ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": user_query})
    with st.chat_message("user"):
        st.markdown(user_query)

    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        progress_bar = st.progress(0, text="Claireê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤...")
        response_placeholder = st.empty() # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ë¹ˆ ê³µê°„
        assistant_response_final = ""
        
        try:
            # 1. ì¥ê¸° ê¸°ì–µ íšŒìƒ (MemorySystem ì‚¬ìš©)
            progress_bar.progress(10, text="ê³¼ê±° ê¸°ì–µì„ íšŒìƒí•˜ëŠ” ì¤‘...")
            recalled_long_term_memories = memory_system_instance.retrieve_relevant_memories(user_query, top_k=1)
            recalled_ltm_str_for_prompt = "\n".join(
                [f"- (ID:{mem.id}, ì¤‘ìš”ë„:{mem.user_importance_score:.2f}) {mem.summary}" for mem in recalled_long_term_memories]
            ) if recalled_long_term_memories else "íšŒìƒëœ ì£¼ìš” ê³¼ê±° ëŒ€í™” ì—†ìŒ."

            # 2. RAG ë¬¸ì„œ ê²€ìƒ‰ (VectorStoreRetriever ì‚¬ìš©)
            progress_bar.progress(30, text="ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘ (RAG)...")
            rag_context_str_for_prompt = ""
            if rag_vector_store_global: # RAG ë²¡í„° ìŠ¤í† ì–´ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
                try:
                    rag_retriever = rag_vector_store_global.as_retriever(search_kwargs={"k": 2}) # ìƒìœ„ 2ê°œ ë¬¸ì„œ ê²€ìƒ‰
                    retrieved_rag_documents = rag_retriever.invoke(user_query)
                    rag_context_str_for_prompt = "\n\n".join(
                        [f"ë¬¸ì„œ ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}\në‚´ìš©: {doc.page_content[:250]}..." for doc in retrieved_rag_documents]
                    ) if retrieved_rag_documents else "í˜„ì¬ ì§ˆì˜ì™€ ê´€ë ¨ëœ ì™¸ë¶€ ì°¸ì¡° ì •ë³´ ì—†ìŒ."
                except Exception as e_rag:
                    st.warning(f"RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_rag}")
                    rag_context_str_for_prompt = "RAG ì •ë³´ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            else:
                rag_context_str_for_prompt = "RAG ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨)."

            # 3. LLMì— ì „ë‹¬í•  ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            progress_bar.progress(50, text="AIì—ê²Œ ì „ë‹¬í•  ì •ë³´ë¥¼ ì¢…í•©í•˜ëŠ” ì¤‘...")
            current_time_for_prompt = datetime.datetime.now().strftime("%Yë…„ %mì›” %dì¼ %A %p %I:%M")
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë‚´ìš© í¬ë§¤íŒ…
            # str()ë¡œ ê°ì‹¸ì„œ Noneì¼ ê²½ìš° "None" ë¬¸ìì—´ì´ ë“¤ì–´ê°€ì§€ ì•Šë„ë¡ ë°©ì§€ (íŠ¹íˆ recalled_ltm_str, rag_context_str)
            formatted_system_content = SYSTEM_PROMPT_CONTENT_TEMPLATE.format(
                current_datetime=str(current_time_for_prompt),
                current_location=str(CURRENT_LOCATION_STR),
                user_name=str(st.session_state.user_profile.get("name", "ì‚¬ìš©ì")),
                user_profile_summary=str(st.session_state.user_profile.get("preferences_summary", "ì •ë³´ ì—†ìŒ")),
                recalled_ltm_str=str(recalled_ltm_str_for_prompt),
                rag_context_str=str(rag_context_str_for_prompt)
            )
            system_message_object = SystemMessage(content=formatted_system_content)

            # LangChain ë©”ëª¨ë¦¬ì—ì„œ ì´ì „ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
            short_term_memory_messages: List[BaseMessage] = st.session_state.lc_memory.chat_memory.messages
            
            # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€ ê°ì²´
            human_message_object = HumanMessage(content=user_query)

            # LLMì— ì „ë‹¬í•  ì „ì²´ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            # ìˆœì„œ: ì‹œìŠ¤í…œ ë©”ì‹œì§€ -> ë‹¨ê¸° ê¸°ì–µ(ê³¼ê±° ëŒ€í™”) -> í˜„ì¬ ì‚¬ìš©ì ì…ë ¥
            final_messages_for_llm: List[BaseMessage] = [system_message_object] + short_term_memory_messages + [human_message_object]
            
            # --- ë””ë²„ê¹…ìš© í”„ë¦°íŠ¸ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ) ---
            # print("\n--- DEBUG: ìµœì¢… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (LLM ì „ë‹¬ ì§ì „) ---")
            # for i, msg_obj in enumerate(final_messages_for_llm):
            #     print(f"Msg {i}: type={type(msg_obj).__name__}, content='{msg_obj.content[:200]}...'")
            # print("--- END DEBUG ---")
            # --- ë””ë²„ê¹…ìš© í”„ë¦°íŠ¸ ë ---

            # 4. ChatOllama ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            progress_bar.progress(70, text="Claireê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...")
            
            stream_from_llm = current_chat_llm.stream(final_messages_for_llm) # ChatOllamaì˜ stream ë©”ì„œë“œ ì‚¬ìš©
            for chunk in stream_from_llm: # chunkëŠ” AIMessageChunk ê°ì²´
                assistant_response_final += chunk.content
                response_placeholder.markdown(assistant_response_final + "â–Œ") # íƒ€ì´í•‘ íš¨ê³¼
            
            response_placeholder.markdown(assistant_response_final) # ìµœì¢… ì‘ë‹µ í‘œì‹œ

        except Exception as e_main_chat:
            st.error(f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_main_chat}")
            # traceback.print_exc() # ê°œë°œ ì‹œ ìƒì„¸ ì˜¤ë¥˜ í™•ì¸ìš©
            assistant_response_final = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            response_placeholder.markdown(assistant_response_final)
        
        finally:
            progress_bar.empty() # ì§„í–‰ í‘œì‹œì¤„ ì œê±°

    # UI ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ë° LangChain ë©”ëª¨ë¦¬ì— AI ì‘ë‹µ ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": assistant_response_final})
    st.session_state.lc_memory.save_context(
        {"human_input": user_query}, # LangChain ë©”ëª¨ë¦¬ì˜ input_key
        {"output": assistant_response_final} # LangChain ë©”ëª¨ë¦¬ì˜ output_key (ê¸°ë³¸ê°’)
    )

    # ìë™ ì¥ê¸° ê¸°ì–µ ì €ì¥ ê¸°ëŠ¥ (í™œì„±í™”ëœ ê²½ìš°)
    if st.session_state.get("auto_save_ltm_checkbox", False):
        with st.spinner("ìë™ ì¥ê¸° ê¸°ì–µ ì €ì¥ ì¤‘ (LLM ìš”ì•½ ì‚¬ìš©)..."):
            # í˜„ì¬ê¹Œì§€ì˜ ì „ì²´ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ (st.session_state.messages ì‚¬ìš©)
            current_conversation_history_for_auto_save = "\n".join(
                [f"{m['role']}: {m['content']}" for m in st.session_state.messages if m['role'] != 'system']
            )
            if current_conversation_history_for_auto_save.strip():
                print(f"Auto-saving LTM for session: {st.session_state.current_session_id}")
                auto_saved_entry = memory_system_instance.consolidate_session_memory(
                    session_id=st.session_state.current_session_id, # í˜„ì¬ ì„¸ì…˜ ID ì‚¬ìš©
                    conversation_history_str=current_conversation_history_for_auto_save,
                    use_llm_summary=True # ìë™ ì €ì¥ì€ í•­ìƒ LLM ìš”ì•½ ì‚¬ìš© (ë˜ëŠ” ë‹¤ë¥¸ ê·œì¹™ ì ìš© ê°€ëŠ¥)
                )
                if auto_saved_entry:
                    st.toast(f"ëŒ€í™” ë‚´ìš©ì´ ìë™ìœ¼ë¡œ ì¥ê¸° ê¸°ì–µì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (ID: {auto_saved_entry.id}).", icon="ğŸ’¾")
                else:
                    st.toast("ìë™ ì¥ê¸° ê¸°ì–µ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", icon="âš ï¸")